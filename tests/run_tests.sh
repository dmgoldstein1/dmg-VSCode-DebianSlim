#!/bin/bash

# Generated by Copilot (GPT-4o), 2025-08-30
# Description: Master test runner for the VS Code Debian Slim Docker build system
# Orchestrates all test types: unit, integration, extension, and security tests

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$SCRIPT_DIR/.."
LOGS_DIR="$PROJECT_ROOT/logs"
TEST_LOG="$LOGS_DIR/test-runner-$(date +%s).log"

mkdir -p "$LOGS_DIR"

# Color output functions
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() {
    echo -e "${BLUE}[INFO]${NC} $*" | tee -a "$TEST_LOG"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $*" | tee -a "$TEST_LOG"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $*" | tee -a "$TEST_LOG"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $*" | tee -a "$TEST_LOG"
}

# Test execution functions
run_unit_tests() {
    log_info "Running unit tests..."
    
    cd "$SCRIPT_DIR/unit"
    if ./unit_tests.sh; then
        log_success "Unit tests passed"
        return 0
    else
        log_error "Unit tests failed"
        return 1
    fi
}

run_integration_tests() {
    log_info "Running integration tests..."
    
    cd "$SCRIPT_DIR/integration"
    if ./test_build_script.sh; then
        log_success "Integration tests passed"
        return 0
    else
        log_error "Integration tests failed"
        return 1
    fi
}

run_extension_tests() {
    log_info "Running VS Code extension tests..."
    
    # Check if npm is available
    if ! command -v npm >/dev/null 2>&1; then
        log_warning "npm not found, skipping extension tests"
        return 0
    fi
    
    cd "$PROJECT_ROOT/vscode-container-updater"
    
    # Install dependencies if needed
    if [[ ! -d "node_modules" ]]; then
        log_info "Installing npm dependencies..."
        npm ci
    fi
    
    # Compile TypeScript
    log_info "Compiling TypeScript..."
    if npm run compile; then
        log_success "TypeScript compilation passed"
    else
        log_error "TypeScript compilation failed"
        return 1
    fi
    
    # Package extension
    log_info "Packaging extension..."
    if npx @vscode/vsce package --out test-package.vsix --allow-missing-repository; then
        log_success "Extension packaging passed"
        # Clean up test package
        rm -f test-package.vsix
    else
        log_error "Extension packaging failed"
        return 1
    fi
    
    return 0
}

run_security_tests() {
    log_info "Running security tests..."
    
    local security_failures=0
    
    # ShellCheck all shell scripts
    log_info "Running ShellCheck on shell scripts..."
    cd "$PROJECT_ROOT"
    if find . -name "*.sh" -type f -exec shellcheck {} +; then
        log_success "ShellCheck passed"
    else
        log_error "ShellCheck found issues"
        ((security_failures++))
    fi
    
    # Check for potential secrets (focusing on actual hardcoded values)
    log_info "Scanning for potential hardcoded secrets..."
    # Look for patterns that suggest actual secret values, not just variable names
    secret_patterns_found=false
    
    # Check for actual secret-like patterns (not just variable names)
    if grep -r -E "(api_key|secret|token|password)[[:space:]]*=[[:space:]]*[\"'][^\"']{20,}[\"']" \
        --include="*.sh" --include="*.yaml" --include="*.yml" \
        --exclude-dir=".git" --exclude-dir="node_modules" \
        --exclude-dir="logs" . 2>/dev/null; then
        secret_patterns_found=true
    fi
    
    # Check for GitHub token patterns that look real
    if grep -r -E "gh[ps]_[A-Za-z0-9]{36,}" \
        --include="*.sh" --include="*.yaml" --include="*.yml" \
        --exclude-dir=".git" --exclude-dir="node_modules" \
        --exclude-dir="logs" . 2>/dev/null | grep -v "PLACEHOLDER" | grep -v "example"; then
        secret_patterns_found=true
    fi
    
    if $secret_patterns_found; then
        log_warning "Potential hardcoded secrets detected - please review"
        ((security_failures++))
    else
        log_success "No hardcoded secrets detected"
    fi
    
    # Validate YAML files
    log_info "Validating YAML files..."
    if command -v yamllint >/dev/null 2>&1; then
        if find . \( -name "*.yaml" -o -name "*.yml" \) \
           -not -path "./vscode-container-updater/node_modules/*" \
           -not -path "./*/node_modules/*" \
           -print0 | xargs -0 yamllint -d relaxed; then
            log_success "YAML validation passed"
        else
            log_error "YAML validation failed"
            ((security_failures++))
        fi
    else
        log_warning "yamllint not installed, skipping YAML validation"
    fi
    
    if [[ $security_failures -eq 0 ]]; then
        log_success "Security tests passed"
        return 0
    else
        log_error "Security tests failed with $security_failures issues"
        return 1
    fi
}

# Usage function
show_usage() {
    cat << EOF
Usage: $0 [OPTIONS] [TEST_TYPE]

Test runner for VS Code Debian Slim Docker build system

TEST_TYPE:
    unit            Run unit tests only
    integration     Run integration tests only
    extension       Run VS Code extension tests only
    security        Run security tests only
    all             Run all tests (default)

OPTIONS:
    -h, --help      Show this help message
    -v, --verbose   Enable verbose output
    --no-cleanup    Don't clean up test artifacts
    --parallel      Run tests in parallel where possible

Examples:
    $0                  # Run all tests
    $0 unit             # Run only unit tests
    $0 integration      # Run only integration tests
    $0 --verbose all    # Run all tests with verbose output

EOF
}

# Parse command line arguments
VERBOSE=false
NO_CLEANUP=false
PARALLEL=false
TEST_TYPE="all"

while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_usage
            exit 0
            ;;
        -v|--verbose)
            VERBOSE=true
            export VERBOSE
            set -x
            shift
            ;;
        --no-cleanup)
            NO_CLEANUP=true
            shift
            ;;
        --parallel)
            PARALLEL=true
            shift
            ;;
        unit|integration|extension|security|all)
            TEST_TYPE="$1"
            shift
            ;;
        *)
            log_error "Unknown option: $1"
            show_usage
            exit 1
            ;;
    esac
done

# Main test execution
main() {
    log_info "Starting test runner..."
    log_info "Test type: $TEST_TYPE"
    log_info "Project root: $PROJECT_ROOT"
    log_info "Logs directory: $LOGS_DIR"
    log_info "Test log: $TEST_LOG"
    
    local test_failures=0
    local start_time
    start_time=$(date +%s)
    
    case "$TEST_TYPE" in
        unit)
            run_unit_tests || ((test_failures++))
            ;;
        integration)
            run_integration_tests || ((test_failures++))
            ;;
        extension)
            run_extension_tests || ((test_failures++))
            ;;
        security)
            run_security_tests || ((test_failures++))
            ;;
        all)
            if [[ "$PARALLEL" == "true" ]]; then
                log_info "Running tests in parallel..."
                (
                    run_unit_tests || echo "UNIT_FAILED" > "$LOGS_DIR/unit_result"
                ) &
                (
                    run_security_tests || echo "SECURITY_FAILED" > "$LOGS_DIR/security_result"
                ) &
                wait
                
                # Check parallel results
                [[ -f "$LOGS_DIR/unit_result" ]] && ((test_failures++))
                [[ -f "$LOGS_DIR/security_result" ]] && ((test_failures++))
                
                # Clean up result files
                rm -f "$LOGS_DIR/unit_result" "$LOGS_DIR/security_result"
                
                # Run sequential tests
                run_extension_tests || ((test_failures++))
                run_integration_tests || ((test_failures++))
            else
                log_info "Running tests sequentially..."
                run_unit_tests || ((test_failures++))
                run_extension_tests || ((test_failures++))
                run_security_tests || ((test_failures++))
                run_integration_tests || ((test_failures++))
            fi
            ;;
    esac
    
    local end_time
    end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    # Final summary
    echo ""
    log_info "==================== TEST SUMMARY ===================="
    log_info "Test type: $TEST_TYPE"
    log_info "Duration: ${duration}s"
    log_info "Failures: $test_failures"
    
    if [[ $test_failures -eq 0 ]]; then
        log_success "üéâ All tests passed!"
        log_info "View detailed logs: $TEST_LOG"
    else
        log_error "‚ùå $test_failures test(s) failed!"
        log_info "Check logs for details: $TEST_LOG"
    fi
    
    # Cleanup
    if [[ "$NO_CLEANUP" != "true" ]]; then
        log_info "Cleaning up test artifacts..."
        # Add cleanup commands here if needed
    fi
    
    exit $test_failures
}

# Trap to ensure cleanup on exit
trap 'log_info "Test runner interrupted"' INT TERM

main "$@"
